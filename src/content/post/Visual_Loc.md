---
title: "Autolocalizaci√≥n Visual Basada en Marcadores"
description: "En esta pr√°ctica se pretende estimar la posici√≥n y orientaci√≥n de un robot en un entorno 2D mediante la detecci√≥n de marcadores visuales AprilTags, aplicando t√©cnicas de visi√≥n por computadora y transformaciones geom√©tricas."
publishDate: "18 MAY 2025"
tags: [
  "rob√≥tica",
  "visi√≥n por computadora",
  "localizaci√≥n visual",
  "AprilTags",
  "estimaci√≥n de pose"
]
coverImage:
  src: "./images_post/AprilTags/portada.png"
  alt: "Reconstrucci√≥n 3D mediante marcadores visuales"
  width: 1200  # Sin unidades (se asume px)
  height: 600
draft: false
---

## Sistema de Posicionamiento üß≠
El uso de marcadores fiduciales como AprilTags permite obtener una localizaci√≥n precisa y robusta mediante t√©cnicas de visi√≥n artificial. Esta estrategia se basa en la detecci√≥n de marcadores visuales cuya geometr√≠a y posici√≥n en el mundo son conocidas de antemano. A partir de su aparici√≥n en la imagen captada por la c√°mara del robot, es posible estimar la pose relativa del marcador respecto a la c√°mara utilizando algoritmos como solvePnP.

Gracias a esta estimaci√≥n y a la cadena de transformaciones geom√©tricas, es posible inferir directamente la posici√≥n del robot en el entorno global. De esta forma, se logra un sistema de autolocalizaci√≥n visual sin necesidad de construir o cargar un mapa del entorno, , lo que permite una puesta en marcha m√°s directa y flexible en entornos controlados.

La Figura 1, tomada del trabajo de Zhang et al. (2023) sobre localizaci√≥n visual en rob√≥tica agr√≠cola, resume el esquema geom√©trico general para la estimaci√≥n de la pose de un veh√≠culo m√≥vil. Este planteamiento te√≥rico representa el fundamento del reto abordado en esta pr√°ctica.

![Info April Tags](./images_post/AprilTags/info_apriltags_redimensionada.png)
**_Figura 1_**: Descripci√≥n del sistema de coordenadas para la localizaci√≥n del robot.\
_[Zhang, Wei & Gong, Liang & Sun, Yefeng & Gao, Bishu & Yu, Chenrui & Liu, Chengliang. (2023). Precise visual positioning of agricultural mobile robots with a fiducial marker reprojection approach. Measurement Science and Technology. 34. 10.1088/1361-6501/ace8b0. ]_

Por tanto, a priori, la estimaci√≥n de la pose del robot se basa en una cadena de transformaciones que relaciona los distintos sistemas de referencia involucrados.

```math
RT_mundo_robot = RT_mundo_tag¬∑RT_tag_camara¬∑RT_camara_robot
```

## Detecci√≥n de Marcadores AprilTags üéØ

El primer desaf√≠o a abordar es la detecci√≥n de los marcadores AprilTags. Para ello, se emplea el detector _pyapriltags_, capaz de identificar en tiempo real las esquinas de cada marcador y extraer su identificador. Gracias a esta informaci√≥n, es posible acceder a la pose absoluta del marcador en el sistema de coordenadas global obteniendo su posici√≥n y orientaci√≥n en el entorno. 

Para facilitar la depuraci√≥n visual y comprobar que la detecci√≥n de marcadores se realiza correctamente, se han a√±adido varios colores a los bordes de los marcadores detectados.

![Colours April Tags](./images_post/AprilTags/tags_colours.png)

Una vez detectado un marcador v√°lido, y conociendo su geometr√≠a real en el mundo, se procede a la obtenci√≥n de la primera transformaci√≥n, la pose del marcador respecto a la c√°mara. Para ello, se necesitan las coordenadas 3D reales de las esquinas del marcador, las coordenadas 2D de esas esquinas obtenidas por el detector y La matriz de par√°metros intr√≠nsecos de la c√°mara. 

Dicha matriz se ha construido a partir de los valores proporcionados por el simulador. 

**Comando: Matriz de intr√≠nsecos.**
```python
    ros2 topic echo /turtlebot3/camera/camera_info
``` 
Con estos par√°metros y asumiendo una lente sin distorsi√≥n, se resuelve el problema PnP mediante _cv2.solvePnP_, obteniendo los vectores de rotaci√≥n y traslaci√≥n que permiten construir la matriz **RT_tag_cam**. 

Para asegurar que la pose calculada con solvePnP es correcta, se realiza una validaci√≥n visual directa. Por un lado, se dibujan los ejes del sistema del marcador sobre la imagen mediante cv2.drawFrameAxes, y adem√°s, permite comprobar su orientaci√≥n. Por otro lado, se proyectan las esquinas 3D reales del marcador sobre la imagen usando cv2.projectPoints, y si estas coinciden con las esquinas detectadas, podemos confiar en que la transformaci√≥n obtenida describe con precisi√≥n la relaci√≥n entre el marcador y la c√°mara.
‚Äã![Verificar RT](./images_post/AprilTags/projected_points.png)

## Cadena de Transformaciones üëì

Al observar los ejes dibujados sobre la imagen del marcador, se aprecia que no se alinean con el sistema de referencia del simulador. Esto ocurre porque solvePnP devuelve la pose en el sistema √≥ptico de OpenCV, donde el eje Y apunta hacia abajo y el Z hacia delante, lo cual no es compatible con el sistema usado para representar el mundo del robot.

Para resolverlo, se define una matriz de transformaci√≥n que permite pasar del sistema √≥ptico al sistema de referencia (**RT_optical_mundo**). Esta matriz rota -90¬∞ alrededor del eje X y luego -90¬∞ alrededor del eje Z, reorientando los ejes para que coincidan la referencia definida, eje X hacia delante, Y hacia la izquierda y Z hacia arriba. 

Adem√°s, la c√°mara se encuentra montada sobre el robot. Por tanto, se introduce tambi√©n la transformaci√≥n entre la c√°mara y el robot, permitiendo estimar la pose final del robot en el mundo.

Todas estas transformaciones se encadenan de forma consistente para describir la pose del robot en el mundo. 

```math
RT_robot_mundo = RT_tag_mundo ¬∑ RT_optical_mundo ¬∑ RT_cam_tag ¬∑ RT_optical_mundo‚Åª¬π ¬∑ RT_robot_cam
```

Dado que se obtiene la RT_tag_cam, y en la cadena de transformaciones se requiere su inversa, es decir, la transformaci√≥n de la c√°mara respecto al marcador, **RT_cam_tag**, el √∫nico elemento restante para completar correctamente el sistema es la transformaci√≥n entre la c√°mara y el robot. Dicha transformaci√≥n, se obtiene mediante el comando mostrado a continuaci√≥n.

**Comando: Matriz de transformaci√≥n C√°mara‚ÄìRobot.**
```python
    ros2 run tf2_ros tf2_echo base_link camera_rgb_frame
``` 

# Estimaci√≥n de la posici√≥n

Con todas las transformaciones necesarias obtenidas se realiza el c√≥mputo final para estimar la pose del robot en el mundo. El resultado es una matriz homog√©nea de 4√ó4 que describe la orientaci√≥n del robot dentro del sistema global de navegaci√≥n, permitiendo la integraci√≥n directa en la tarea de localizaci√≥n.

A partir de la matriz final RT_robot_world, se extraen directamente las coordenadas del robot. La posici√≥n se obtiene del vector de traslaci√≥n, y la orientaci√≥n se calcula por la arcotangente, que corresponde a la rotaci√≥n en el plano.

En caso de no detectarse ning√∫n marcador visible, la estimaci√≥n de la posici√≥n se realiza mediante odometr√≠a. Dado que se conoce la velocidad lineal, el giro angular, se actualiza la pose integrando ambos t√©rminos. Este m√©todo, aunque funcional, est√° m√°s expuesto a la acumulaci√≥n de errores. 

En el v√≠deo adjunto se aprecia c√≥mo la estimaci√≥n deriva progresivamente hasta que vuelve a detectar un marcador, momento en que corrige autom√°ticamente la posici√≥n estimada.

## V√≠deo üé•
1. [Autolocalizaci√≥n visual basada en marcadores apriltags completa.](https://youtu.be/UpFAeQSnzSg)